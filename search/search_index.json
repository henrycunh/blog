{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"whats up?","text":"<p>Currently working as an independent consultant. My use my expertise in recommendation systems to helps fast-growing startups build out their RAG applications. I am also the creator Instructor and Flight, and ML and data science educator.</p>"},{"location":"#writing","title":"Writing","text":"<ul> <li>Rag is more than embeddings</li> <li>Understanding Python: Caching</li> <li>Understanding Python: Async</li> <li>Flight, a framework for building and executing pipelines</li> <li>LLM Observability is just plain observability</li> </ul>"},{"location":"#clients","title":"Clients","text":"<ul> <li>Rewind AI: Rewind AI is a personal memory assistant that helps you remember, organize, and navigate your life.</li> <li>Naro: Naro sits in the background, using contextual clues to proactively select the right content and messaging for each customer conversation, from emails to meetings.</li> <li>Trunk Tools: Trunk Tools addresses the skilled labor shortage in construction, enhancing workforce productivity through AI-based tools.</li> <li>Modal Labs: Modal specializes in cloud functions, offering a platform for running generative AI models, large-scale batch jobs, and more.</li> <li>Pydantic: Pydantic provides data validation and settings management using Python type annotations, enforcing type hints at runtime with user-friendly error handling.</li> <li>Weights &amp; Biases: Wandb provides a platform for tracking machine learning experiments, offering tools for visualization, comparison, and collaboration in ML projects.</li> <li>New Computer: Dot by New Computer is an intelligent guide designed to help you remember, organize, and navigate your life.</li> <li>Kay.ai: Retrieve relevant context from the semantic web for your LLM apps with fully hosted embeddings.</li> </ul>"},{"location":"#work-history","title":"Work History","text":"<ul> <li>Sabbatical @ South Park Commons - 2023 - Present</li> <li>Staff Machine Learning Engineer @ Stitchfix \u2014 2016, 2018-2023</li> <li>Prev, Meta, ActionIQ, NYU, Meltwater - 2013-2018</li> <li>Computational Mathematics and Statistics @ University of Waterloo</li> </ul>"},{"location":"contact/","title":"Contact","text":"<p>I work with fast growing startups who are looking to invest in their applied ai work. If you're interested in working together, feel free to reach out me via email or twitter. I'm currently fully booked until February 2024.</p> <p>I'm also available for speaking engagements. If you're interested in having me speak at your event or podcast, please reach out via email as well.</p>"},{"location":"writing/","title":"Writing and mumblings","text":"<p>I don't focus on maintaining high writing quality; instead, I prioritize writing frequently. My goal is to maximize the number of words written per month, with the hope that eventually, impressions and impact will align. Some of these links will be videos, some will be tweets, and some will be blog posts. I hope you find something valuable.</p>"},{"location":"writing/#questions","title":"Questions?","text":"<ul> <li>If you have topics you'd like me to write about leave a comment in my discussions</li> </ul>"},{"location":"writing/#writing","title":"Writing","text":"<ul> <li>Advice for young people</li> </ul>"},{"location":"writing/#systems","title":"Systems","text":"<ul> <li>How to build a terrible RAG system</li> <li>Rag is more than embeddings</li> <li>Flight, a framework for building and executing pipelines</li> <li>LLM Observability is just plain observability</li> </ul>"},{"location":"writing/2024/06/01/advice-to-young-people/","title":"Advice to Young People, The Lies I Tell Myself","text":"<p>I'm really not qualified to give advice. But enough people DM'd me on Twitter, so here it is. I don't have to answer the same question over and over again. After some more editing I realised that I am actually write this for my younger Katherine.</p> <p>If you want to know who I am, check out blog/whoami or my Twitter.</p> <p>Don't read this if you're seeking a nuanced perspective</p> <p>These are simply the lies I tell myself to keep on living my life in good faith. I'm not saying this is the right way to do things. I'm just saying this is how I did things. I will do my best to color my advice with my own experiences, but I'm not going to pretend that the the suffering and the privilege I've experienced is universal.</p> <p>Choosing</p> <p>You'll notice that I use the word \"choosing\" frequently. I genuinely believe that we are always making choices and that we have the ability to choose. Choosing can be terrifying because it means we are accountable for our decisions, and there are infinite options before us. It is also frightening because once we have made a decision, we must live with it, it is the death of optionality. But I believe that choosing is the only way to live authentically.</p> <ul> <li> <p>Existential Despair: A feeling of hopelessness rooted in the existentialist belief that life lacks inherent meaning. This despair arises from the realization of one's absolute freedom and the responsibility for creating one's own essence and purpose.</p> </li> <li> <p>Anguish: In existentialism, anguish is the emotional response to recognizing the vastness of one's freedom and the accompanying responsibility for one's actions. It involves the realization that each decision shapes one's essence and affects others, leading to a deep sense of moral responsibility.</p> </li> </ul> <p>There's a joke I once heard about a philosopher. Before going to bed, he wonders if he will be thirsty during the night. So, he goes to the kitchen and places two cups beside his bed: one filled with water and the other left empty, just in case he doesn't want any water.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#how-to-be-lucky","title":"How to Be Lucky","text":"<p>You make your own luck. There's a great experiment that I can't cite, but it has stuck in my mind since I was a child. They identified people as lucky and unlucky, and asked them to count the number of photographs in a newspaper. The unlucky people took a long time to count the photographs, while the lucky people took a very short time. The reason is that the unlucky people were so focused on counting the photographs that they missed the giant text that said, \"Stop counting, there are 43 photographs in this newspaper.\"</p> <p>What I took away from this experiment was the idea that it might not be the case that lucky people and unlucky people have different opportunities, but rather that their field of perception is wider. Lucky people can actually see the opportunities. A lucky person and an unlucky person might meet the same businessman, but they might talk about different things. One could be presented with or ask for an opportunity that the unlucky person doesn't even see as possible.</p> <p>I often ask myself, \"Okay, I'm focused on getting X, but let's not forget to read the headlines.\"</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#how-to-get-a-job","title":"How to Get a Job","text":"<p>I've never gotten a job by applying to it. It's always been referrals or someone reaching out to me. So honestly, my resume is shit compared to my peers. I'm terrible at interviewing, and I've never done leetcode. This is not a brag; it's just not my style. Am I a nepo baby? I don't know. Was I a morale hire? I'm pretty disagreeable. Was it merit? Also not sure.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#merit","title":"Merit","text":"<p>Very few people get a job on merit alone. You have to be a big fish in a big pond. That's like IOI, top 1% in your class, etc. That's just... Sort by grades and interview? That's hard. You literally have to be the best.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#high-agency","title":"High Agency","text":"<p>When I hire someone, I simply want to know that they are capable of taking charge of their own life. It's quite common for people to DM me saying, \"Hey man, how can I help you?\" However, I often find that they haven't put any effort into thinking about it. I usually ignore such messages unless I truly believe they can provide assistance.</p> <p>??? note \"Links that are everywhere on my digital footprint</p> <pre><code>- [Contributing to jxnl/instructor](https://jxnl.github.io/instructor/contributing/)\n- [Contributing to jxnl/instructor-js](https://instructor-ai.github.io/instructor-js/contributing/)\n- [Issues of jxnl/instructor](https://github.com/jxnl/instructor/issues)\n- [Issues of jxnl/instructor-js](https://github.com/instructor-ai/instructor/issues)\n</code></pre> <p>There have been many times people will DM me offering to help while I am writing a blog post. I'll link the blog PR and say 'let me know what you think'. No comments, and then I get ghosted?</p> <p>How to Reach Out</p> <p>Do not send me anything longer than you would send to a crush. Some people email me six-paragraph essays about the time they saved a cat from a tree.</p> <p>I find the most effective way to get someone's attention is to simply give, just like in dating. Hey, I noticed that you read this book on your website. I think you'd like this book too; it's pretty short, etc. Oh, I noticed some tests were failing in your repository. I fixed them for you. Hey! I've added some examples to the codebase. Do you have any feedback?</p> <p>There are so many little ways to get people's attention that aren't self-centered. I do this with my consulting too. During the first call, my only goal is to tell you everything I think could help your business. I don't care if you hire me or not. I just want to help. And as it turns out, this leads me to the next part.</p> <p>I recognize that not everyone has access to the same networking opportunities, and the traditional job application process can be a valid and necessary path for many. But social media when used correctly is a great way to get a opportunity</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#be-the-plumber","title":"Be the Plumber","text":"<p>When my toilet is overflowing with shit and my wife is about to come home in 2 hours, and I find a plumber, the plumber does not go, \"OMG, thank you for this opportunity.\" They are past that. They know that you're in a pinch, and they know that what they have is valuable. They know they are here to solve MY problem. Hiring is the same way. This is why people want to hire senior folks because there's some trust.</p> <p></p> <p>But ultimately, you have to understand that unless there's some tremendous tax break and positive EV, the people who are hiring NEED HELP. Your job is to help them solve the problem or find the problem to solve. You're here to solve my problem; you're not here to collect charity work.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#be-someone-people-want-to-work-with","title":"Be Someone People Want to Work With","text":"<p>If your metric for working somewhere is being someone people want to work with, it turns out skill is not the highest metric. I've seen people get hired because they're fun to be around. If you want to be the smartest person, then yeah, good grades and being the smartest person in the room is a good metric. But if you want to get a job, \"man, you're great to be around\" is a very strong metric. This is obviously conditional on skill, and you should obviously focus on skills acquisition.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#imposter-syndrome","title":"Imposter Syndrome","text":"<p>If I hired you, don't insult me by having imposter syndrome. I hired you because I think you can solve my problem. I didn't hire you to compare you to other plumbers (I might), but at the end of the day, you must just think I have shit taste and that you've somehow tricked me into thinking you're good when you're an imposter? Right?</p> <p>If you don't believe in yourself, believe in me that believes in you.</p> <p></p>"},{"location":"writing/2024/06/01/advice-to-young-people/#how-to-be-good-at-many-things","title":"How to Be Good at Many Things","text":"<p>I think too many people reading this are mostly pursuing intellectual activities, but I'm mostly gonna focus on using analogies of personal physical fitness to describe how I think about this kind of stuff.</p> <p>In the beginning, you're gonna have a noob gains, just an act of practicing is going to be enough to make improvements. You're gonna be so weak and out of shape that if you wait, it's gonna be cardio, and if you run, you're gonna get stronger. And it's gonna be months or years of seeing progress just by showing up every day and doing anything. You're just gonna be making progress. Maybe you're also \"gifted\" and so things come easier to you, but there's gonna be a time when things change.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#cost-of-being-a-champion","title":"Cost of Being a Champion","text":"<p>The story I tell myself is one where Travis Stevens talks about what it takes to be a judo champion. Something along the lines of \"You think you want to be a champion until you meet someone who truly knows they want to be a champion.\" Replace \"champion\" with \"comedian,\" \"boxer,\" \"founder,\" and so on.</p> <p>Do you realize what it takes to be a judo champion?</p> <p>You need to sacrifice relationships, break some fingers, go through an ACL tear, and even get a concussion before fighting for the Olympics. You have to endure injuries and wonder if your career is over. You have to get the flu right before nationals and fight through it. And if you lose, you have to refrain from making excuses. That's incredibly challenging, and it's perfectly acceptable to choose not to pursue that path.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#love-plateaus","title":"Love Plateaus","text":"<p>Once you start hitting a plateau, can you just start thinking about progressive overload and making sure what you're doing is facing difficulty at some consistent rate, but ultimately this is just to get you over and through some of these spots. At some point in the future, how much better you get will just be a function of how badly you want it and how much you enjoy the act.</p> <p>How to identify the champ</p> <p>I remember that video where they were asking this Olympic weightlifter what matters more talent or hard work. And he said something along the lines of. \"At the national level, everyone is talented, but when you by the time you go to the olympics hard work is the only thing that matters, there are many injuries and challenges, and the people who get it are the people who want it the most\"</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#on-season-and-off-season","title":"On Season and Off Season","text":"<p>I think a lot of people ask me specifically about this question because I do so many things. I'm a martial artist, assistant, free diver, blah blah blah blah blah. I don't think I'm a leader at any of these things, but I do think I'm elite at learning. And just like there is an off-season and on-season for sports, I think there's an off-season and on-season for learning. And I think that's the most important thing to understand.</p> <p>At some point, cardio will hurt your strength gains. At some point, strength gains will hurt your cardio. At some point, you're gonna have to choose between being a powerlifter or a marathon runner. And that's okay. You can always come back to it. But I think the most important thing is to understand that you can't be good at everything all the time. And that's okay. You can always come back to it.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#how-to-be-confident","title":"How to Be Confident","text":"<p>I am a deeply insecure person, and I think my therapist would probably agree that I have low self-esteem. But I am confident just the same. I believe it's important to understand that confidence is the ability to believe despite feeling uneasy.</p> <p>Courage is not the absence of fear, but rather taking action despite the fear.</p> <p>I think this applies to many virtues. So I'll only talk about my own experience with confidence. My confidence simply comes from taking action. I know I outwork everyone around me, even if I have no talent. I know I'm willing to make sacrifices, and I know I can focus my energy on a specific goal. Why? Because I've done it so many times in my life.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#jiu-jitsu","title":"Jiu Jitsu","text":"<p>When I started doing jiu-jitsu, I trained hard five days a week. Every training session was challenging, but it also meant that whenever I went to a new gym or a competition, I knew I could handle it. I had already been through tough situations, and I knew I could overcome them again.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#freediving","title":"Freediving","text":"<p>When I started freediving, I swam for an hour every day, six days a week. At the end of each workout, I would practice my breathing routines and apnea training. By the end of my training session, I could swim 80 meters in a pool without taking a breath and hold my breath for 4 \u00bd minutes. This was important because I knew that during my diving trip, the longest distance I would have to swim was 45 meters and the longest breath-hold would be 2 \u00bd minutes. By the time I went on my diving trip, I felt relaxed and ready to enjoy the water. I knew I could do it because I had already accomplished it.</p> <p>Hitting 30m for the first time</p> <p>My personal record for the deepest dive I've done on a single breath is 30m. It was not a very fast dive, so I ended up being underwater for 3 \u00bd minutes. But how did I feel at the time?</p> <ol> <li>3.5 minutes is a minute shorter than my personal best, so I knew I could do it.</li> <li>During the 10-minute rest before the dive, the goal was to lower my heart rate and relax. I was so relaxed that I fell asleep in the water for 10 minutes, and my heart rate was 43 bpm. I woke up, took the last few breaths, and went down.</li> </ol>"},{"location":"writing/2024/06/01/advice-to-young-people/#public-speaking","title":"Public Speaking","text":"<p>People don't believe this, but my talk at the AI Summit was actually my first public speaking experience. It has over 120k views on YouTube. I was so nervous that I basically blacked out and didn't eat all day until the talk. I have no recollection of the day, and I had so much adrenaline that I was shivering before I got up on stage. Leading up to the talk, I rehearsed it for about two hours each day in the past three days. By the time I got on stage, the words just flowed out of me. I had rehearsed the speech standing up, wearing the same jacket, about 20 times in the past month. I knew I could do it because I had already done it.</p> <p>In order to keep doing the reps in the past 3 months I've been relentlessly trying to find podcasts to speak on to keep doing the reps and practicing.</p> <p>The theme is consistent throughout. I don't think it's the only way to gain confidence, as there could be some delusion at play. But I have simply shown myself, over the past 15 years, that I can overcome difficult challenges and do them repeatedly..</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#choosing-happiness","title":"Choosing happiness","text":"<p>In the short term, you would be much happier if you accepted and admitted to yourself that the reason you don't have what you want is simply because you do not want it badly enough. The sooner you accept that, the happier you'll be. Then the next question is: Do you want to be happy or do you want to achieve what you want? It's not the last question, but it definitely is the next question.</p> <p>For who i think my audience is I really recommended byung chul han's book the burnout society or psychopolitics.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#write-more","title":"Write More","text":"<p>I used to hate writing. I was always incredibly insecure about it. English is my second language, and I was often told that I would never be a good writer because I am an immigrant. I scored poorly on the writing section of the SATs. In fact, I failed the entrance English exam at Waterloo and had to rewrite an English essay, or else I would have been kicked out of college in the first week.</p> <p>This is my first piece of writing</p> <p>I've run a bunch of pinnacle blog post, but this is the first time I'm really speaking from my heart. This whole article took me about an hour to write using speech to text. I almost didn't write this, but then I realize that if it even helped one person feel a little less anxious for one day it would've been positive.</p> <p>It wasn't until November 2023 that I truly appreciated the power of writing. Writing helps me stop dwelling on the same thoughts repeatedly. Moreover, when someone asks for advice, I can simply share a link instead of repeating myself. I have found writing to be incredibly valuable because if someone is not willing to read it, they won't benefit from it. The worst outcome is not having written anything at all; it is failing to help someone who could have benefited from it. I have found that writing is a great way to help others while helping myself exorcise the thoughts from my mind and make room for new ones.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#do-it-scared","title":"Do It Scared","text":"<p>I believe many young people struggle with the insecurity of being intelligent. They often feel the need to prove their intelligence by overthinking, excessively planning, and acquiring knowledge before taking action. However, this approach is a waste of time. The more you research, the more you realize how little you actually know. Instead, you just have to do it, even if you're scared.</p> <p>The theory is for analysis; it is not for action</p> <p>When you learn to draw, just draw what you see. And when we get an art education, the theory simply gives us the language to describe our tastes. The challenge I'm giving myself this year is to learn the trumpet without learning any music theory. I'm just going to play with it like a child. I'm just going to listen to a lot of music and try to make those sounds that I like.</p> <p>There have been enough examples of little Chinese kids being forced to learn the piano for seven years, and the moment their parents stop taking them to class, they stop playing music. I think if you love the world and think it is beautiful, learning physics, for example, would not take away from that beauty it only adds do it. But you study natrual science without loving the natural world, you will only see the world as a machine. Which is just not that fun.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#get-rid-of-complexity","title":"Get Rid of Complexity","text":"<p>I think in the same way when making plans or building systems, the same insecurity of being intelligent leads to overthinking and overcomplicating a system, in the hope that the same complexity will be a proxy for intelligence. I think this is a mistake. The best systems are the simplest ones. Feynman's approach to physics is a great example of this. Famously, Einstein said that if you can't explain something simply, you don't understand it well enough. I think this is true for many things.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#a-great-person-is-not-always-a-good-person","title":"A Great Person is not Always a Good Person","text":"<p>Mike Tyson once said, \"If you're favored by God, you're also favored by the devil.\" It's alright to strive for greatness, but it's equally important to choose goodness. Don't neglect your loved ones for work; time is fleeting. Appreciate people and sincerely express your love for them. Grief is the result of unexpressed love, and the key to avoiding grief is to love abundantly.</p> <p>Some people claim they will grind for ten years and then relax once they find a partner or something similar, but I believe that's nonsense. You won't be able to do it suddenly without practice. Along your journey of hard work, you must consciously practice winding down and being a good person. Otherwise, you'll only address it after experiencing your first divorce.</p> <p>Fatherhood</p> <p>I think fatherhood is a great example of this. I think a lot of people think that they can just be a good father when they have kids. But I think that's a mistake. I think you have to practice being a good father before you have kids. You have to practice being a good husband before you get married.</p> <p>It might be easy to convince myself that a good father is able to take care of the family, but how many times was that an excuse to just work yourself to death is the father who can pay for piano lessons, truly a better man than the father, who can be present at the recital? I just don't think this is something that we can turn on and off so easily.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#self-worth","title":"Self-Worth","text":"<p>For a large part of my life, I held onto the belief that my purpose was to help and teach people. I believed in working hard, generating capital, and taking care of my partner, family, and so on. However, when I got injured, I had to confront the fact that I wasn't able to fulfill those responsibilities. I struggled with the realization that I couldn't work, produce, or take care of my partner and family. It made me question, \"If the ox cannot plow, what is it good for?\" This struggle consumed me during the latter half of my twenties.</p> <p>It is crucial to start practicing self-love. While successful people may attribute their success to their struggles, I believe a significant part of it is simply coming to terms with those struggles. Each person's pain is maximum to them. You have already endured hardships. Seek success because you deserve it and learn to love yourself, rather than punishing yourself for not being good enough.</p> <p>You deserve the life that you desire.</p> <p>I used to write down \"What would I do if I really loved myself?\" Sometimes that's hard, especially for someone who has been through a lot of trauma. It can be difficult to even imagine what that would look like. Instead, I now use inverted thinking. What would I do if I really hated myself? What if I wanted myself to lose out as much as possible? What would I do?</p> <ul> <li>Stay home all day.</li> <li>If my friends don't ask to hang out, don't reach out to them out of fear of rejection.</li> <li>Don't ask the girl out.</li> <li>Never learn to talk to strangers.</li> <li>Work hard and make tons of money but die an unhappy old man with regrets.</li> <li>Neglect my health and my body.</li> <li>Never learn to cook.</li> <li>Be afraid to dance in public.</li> <li>Make excuses and believe other people are more talented than me.</li> <li>Never look at how far I've come and only focus on what I don't have.</li> </ul> <p>Whatever it is, just do the opposite.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#self-talk","title":"Self talk","text":"<p>What are the mantra's you've inhereted from others? How often are you chasing \"The Dream\" and not \"Your Dream?\". Really evaluate what you want and why you want it, and make sure your practice talking to yourself.</p> <p>Happiness is the quality of thoughts</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#social-media","title":"Social Media","text":"<p>You don\u2019t need to believe in yourself to be an artist, you just need to believe in your audience. To believe that it is important to communicate with them. Make sure that if youre goal is to build an audience that you have some message to say. Otherwise it just gets hopeless. Celebrity is the cost you pay to deliver your message, celebrity is to be hated.</p> <p>Celebrity is to be hated</p> <p>Twitter is a place where low status people can feel like high status people. I think the Internet is basically summarized by those words. For most of the people who are fighting online the Internet might be the only place where they have a sense of control. When you approach it from a point of sympathy, even getting hate online is an opportunity to be grateful for the real life friendships that you have, and the digital communities that you've built.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#isolation-is-the-gift-all-others-are-a-test-of-your-endurance","title":"Isolation is the gift, all others are a test of your endurance","text":"<p>Really value your time alone. It is the only time you can truly see yourself. It is the only time you can truly hear yourself. Isolation is time not only away from people, but also media.</p> <p>I believe success truely comes from being yourself, but if you're always surrounded by others the influence is too strong. But if you can truely live in the isolation, you'll be like a still pond. When others project their image onto you, you'll be able to see the ripples clearly.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#the-world-is-a-mirror","title":"The world is a mirror","text":"<p>The only language you have to describe the world is with your own perception. The world is a mirror. Often your fear of judgement might stem from your own judgement of others.</p> <p>Jason goes to the gym</p> <p>I used to be afraid of going to the gym because I was afraid of being judged. I was so focused on the idea of the right workout and the right form. If I went to the gym alone, I would just think, \"Wow, people must think I'm so weak. I can barely squat the bar. I'm so lanky.\" I would look at the fat guy or the skinny guy and think, \"Man, they are just like me, clueless.\"</p> <p>Looking back at that time, I had so much judgment of others, but it was really just a reflection of my own judgment of myself. I was so afraid of being judged, but I was the one judging myself the most.</p> <p>Fast forward 5 years, everyone I see coming into the gym is trying to improve their health. I don't think the same way. I look at these people and think, \"Wow, they are so much braver than me. I see them coming consistently, and I can see they are getting results. Good for them! Oh, wow, they are bringing their friends. Awesome.\"</p> <p>The world looks difference when you fix your own perception.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#you-are-a-mirror","title":"You are a mirror","text":"<p>In the same way that the world is a mirror, you are a mirror. You are a mirror to others. When you accept this you also realize that you are not responsible for others. The people who are negative to you are usually miserable themselves. The sooner your realize this the easier it is to let go of the negativity.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#pessimists-are-losers","title":"Pessimists Are Losers","text":"<p>You impress nobody except yourself when you're a pessimist. It doesn't make you a better friend. It does not make you cool and edgy. It does not make you seem smarter just because you can be critical of everything. The world is neither inherently against you nor for you, so believing one or the other is in bad faith.</p> <p>\u201cHe who blames others has a long way to go on his journey. He who blames himself is halfway there. He who blames no one has arrived.\u201d</p> <p>Survival</p> <p>I think pessimism as a survival mechanism is great, its the alertness that warns us potential outcomes we do not want. It's a way to protect yourself. People are alsos more able to find problems than solutions. It's a good exercise to understand if that energy is productive, can this critique of the work improve the work? Or is it just a way to feel superior or to get sympathy?</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#let-go-of-anticipation","title":"Let Go of Anticipation","text":"<p>Here's a story about how regular people and Buddhist monks anticipated the future. They put both of them into an MRI and told them they were going to get shocked at some random time in the future. What they found was that regular people would have an anxious baseline response minutes before they were going to get shocked, but when they did, it was only slightly higher than the baseline.</p> <p>The monks, however, had no resting anxious anticipatory response. There was no overactive brain activity until the moment of the shock. But when they did feel the shock, it was much higher than the group of regular people. They felt more pain!</p> <p>The lesson I learned from this is that anticipation likely dampens the sensations at the moment life happens, but at the cost dampening the joy as well, while creating a contraction in the present moment. This relates to the pessimism bit. You might think you're winning because you believe the pain was lessened by expecting the worse, but you also lose because you were in anticipation the entire time. On the same token letting go of anticipation of success also allows you to enjoy the moment more, the pressure to succeed is equally a contraction.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#you-were-just-a-child","title":"You were just a child","text":"<p>Stuck with me for a bit. I looked at all the stupid things I had done in the past, and all the things that happened to me that I didn't understand. Then someone said to me \"you were just a child\" and it was honestly the most liberating thing I've ever heard. I was just a child. I didn't know any better. I didn't know.</p> <p>Now, you also have to realize that this applies to everyone else too. Its everyone's first time being a person, including your parents.</p>"},{"location":"writing/2024/06/01/advice-to-young-people/#attention-is-a-gift","title":"Attention is a Gift","text":"<p>The most valuable thing you can give someone is your attention. Talk about the things you love, spend time with the people you love, and give your attention wisely. As I said before, grief is the result of unexpressed love, and the key to avoiding grief is to love abundantly, especially yourself.</p> <p>So attend to the things that matter, and make sure to spend your arrogance while you're still young.</p>"},{"location":"writing/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/","title":"AI Engineer Keynote: Pydantic is all you need","text":"<p>Click here to watch the full talk</p> <p>Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts,</p> <p>I'd genuinely appreciate any feedback on the talk \u2013 every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible.</p>"},{"location":"writing/2023/11/13/learn-async/","title":"Batch Processing using <code>asyncio</code> and <code>Instructor</code>","text":"<p>Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using <code>instructor</code> and learn how to use <code>asyncio.gather</code> and <code>asyncio.as_completed</code> for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using <code>asyncio.Semaphore</code>.</p> <p>Github Example</p> <p>If you want to run the code examples in this article, you can find them on jxnl/instructor</p> <p>We will start by defining an <code>async</code> function that calls <code>openai</code> to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch.</p>"},{"location":"writing/2023/11/13/learn-async/#understanding-asyncio","title":"Understanding <code>asyncio</code>","text":"<p><code>asyncio</code> is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: <code>OpenAI()</code> and <code>AsyncOpenAI()</code>. Today, we will be using the <code>AsyncOpenAI()</code> class, which processes data asynchronously.</p> <p>By utilizing these tools in web applications or batch processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially.</p>"},{"location":"writing/2023/11/13/learn-async/#understanding-async-and-await","title":"Understanding <code>async</code> and <code>await</code>","text":"<p>We will be using the <code>async</code> and <code>await</code> keywords to define asynchronous functions. The <code>async</code> keyword is used to define a function that returns a coroutine object. The <code>await</code> keyword is used to wait for the result of a coroutine object.</p> <p>If you want to understand the deeper details of <code>asyncio</code>, I recommend reading this article by Real Python.</p>"},{"location":"writing/2023/11/13/learn-async/#understanding-gather-vs-as_completed","title":"Understanding <code>gather</code> vs <code>as_completed</code>","text":"<p>In this post we'll show two ways to run tasks concurrently: <code>asyncio.gather</code> and <code>asyncio.as_completed</code>. The <code>gather</code> method is used to run multiple tasks concurrently and return the results as a <code>list</code>. The <code>as_completed</code> returns a <code>iterable</code> is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here.</p>"},{"location":"writing/2023/11/13/learn-async/#example-batch-processing","title":"Example: Batch Processing","text":"<p>In this example, we will demonstrate how to use <code>asyncio</code> for batch processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using <code>asyncio</code>.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables `response_model` in `create` method\nclient = instructor.apatch(AsyncOpenAI()) # (1)!\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_person(text: str) -&gt; Person:\n    return await client.chat.completions.create( # (2)!\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": text},\n        ],\n        response_model=Person,\n    )\n</code></pre> <ol> <li>We use <code>instructor.apatch</code> to patch the <code>create</code> method of <code>AsyncOpenAI</code> to accept a <code>response_model</code> argument. This is because the <code>create</code> method of <code>AsyncOpenAI</code> does not accept a <code>response_model</code> argument without this patch.</li> <li>We use <code>await</code> here to wait for the response from the server before we return the result. This is because <code>create</code> returns a coroutine object, not the result of the coroutine.</li> </ol> <p>Notice that now there are <code>async</code> and <code>await</code> keywords in the function definition. This is because we're using the <code>asyncio</code> library to run the function concurrently. Now lets define a batch of texts to process.</p> <pre><code>dataset = [\n        \"My name is John and I am 20 years old\",\n        \"My name is Mary and I am 21 years old\",\n        \"My name is Bob and I am 22 years old\",\n        \"My name is Alice and I am 23 years old\",\n        \"My name is Jane and I am 24 years old\",\n        \"My name is Joe and I am 25 years old\",\n        \"My name is Jill and I am 26 years old\",\n    ]\n</code></pre>"},{"location":"writing/2023/11/13/learn-async/#for-loop-running-tasks-sequentially","title":"<code>for loop</code>: Running tasks sequentially.","text":"<pre><code>persons = []\nfor text in dataset:\n    person = await extract_person(text)\n    persons.append(person)\n</code></pre> <p>Even though there is an <code>await</code> keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a <code>for</code> loop to iterate over the dataset. This method, which uses a <code>for</code> loop, will be the slowest among the four methods discussed today.</p>"},{"location":"writing/2023/11/13/learn-async/#asynciogather-running-tasks-concurrently","title":"<code>asyncio.gather</code>: Running tasks concurrently.","text":"<pre><code>async def gather():\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    all_persons = await asyncio.gather(*tasks_get_persons) # (1)!\n</code></pre> <ol> <li>We use <code>await</code> here to wait for all the tasks to finish before assigning the result to <code>all_persons</code>. This is because <code>asyncio.gather</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.as_completed</code> to achieve the same result.</li> </ol> <p>Using <code>asyncio.gather</code> allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where <code>asyncio.as_completed</code> comes into play.</p>"},{"location":"writing/2023/11/13/learn-async/#asyncioas_completed-handling-tasks-as-they-complete","title":"<code>asyncio.as_completed</code>: Handling tasks as they complete.","text":"<pre><code>async def as_completed():\n    all_persons = []\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person) # (1)!\n</code></pre> <ol> <li>We use <code>await</code> here to wait for each task to complete before appending it to the list. This is because <code>as_completed</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.gather</code> to achieve the same result.</li> </ol> <p>This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client.</p> <p>However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make.</p> <p>Ordering of results</p> <p>Its important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use <code>asyncio.gather</code> instead.</p>"},{"location":"writing/2023/11/13/learn-async/#rate-limited-gather-using-semaphores-to-limit-concurrency","title":"Rate-Limited Gather: Using semaphores to limit concurrency.","text":"<pre><code>sem = asyncio.Semaphore(2)\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -&gt; Person:\n    async with sem: # (1)!\n        return await extract_person(text)\n\nasync def rate_limited_gather(sem: Semaphore):\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    resp = await asyncio.gather(*tasks_get_persons)\n</code></pre> <ol> <li>We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to.</li> </ol>"},{"location":"writing/2023/11/13/learn-async/#rate-limited-as-completed-using-semaphores-to-limit-concurrency","title":"Rate-Limited As Completed: Using semaphores to limit concurrency.","text":"<pre><code>sem = asyncio.Semaphore(2)\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -&gt; Person:\n    async with sem: # (1)!\n        return await extract_person(text)\n\nasync def rate_limited_as_completed(sem: Semaphore):\n    all_persons = []\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person) # (2)!\n</code></pre> <ol> <li> <p>We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to.</p> </li> <li> <p>We use <code>await</code> here to wait for each task to complete before appending it to the list. This is because <code>as_completed</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.gather</code> to achieve the same result.</p> </li> </ol> <p>Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced.</p> <p>Other Options</p> <p>Its important to also note that here we are using a <code>semaphore</code> to limit the number of concurrent requests. However, there are other ways to limit concurrency esp since we have rate limit information from the <code>openai</code> request. You can imagine using a library like <code>ratelimit</code> to limit the number of requests per second. OR catching rate limit exceptions and using <code>tenacity</code> to retry the request after a certain amount of time.</p> <ul> <li>tenacity</li> <li>aiolimiter</li> </ul>"},{"location":"writing/2023/11/13/learn-async/#results","title":"Results","text":"<p>As you can see, the <code>for</code> loop is the slowest, while <code>asyncio.as_completed</code> and <code>asyncio.gather</code> are the fastest without any rate limiting.</p> Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as_completed 3.26 seconds 2"},{"location":"writing/2023/11/13/learn-async/#practical-implications-of-batch-processing","title":"Practical implications of batch processing","text":"<p>The choice of approach depends on the task's nature and the desired balance between speed and resource utilization.</p> <p>Here are some guidelines to consider:</p> <ul> <li>Use <code>asyncio.gather</code> for handling multiple independent tasks quickly.</li> <li>Apply <code>asyncio.as_completed</code> for large datasets to process tasks as they complete.</li> <li>Implement rate-limiting to avoid overwhelming servers or API endpoints.</li> </ul> <p>If you find the content helpful or want to try out <code>Instructor</code>, please visit our GitHub page and give us a star!</p>"},{"location":"writing/2023/11/26/python-caching/","title":"Caching in Python","text":"<p>Instructor makes working with language models easy, but they are still computationally expensive.</p> <p>Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with <code>pickle</code>, and explore solutions that use <code>decorators</code> like <code>functools.cache</code>. Then, we'll craft custom decorators with <code>diskcache</code> and <code>redis</code> to support persistent caching and distributed systems.</p> <p>Lets first consider our canonical example, using the <code>OpenAI</code> Python client to extract user details.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Enables `response_model`\nclient = instructor.patch(OpenAI())\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": data},\n    ]\n)\n</code></pre> <p>Now imagine batch processing data, running tests or experiments, or simply calling <code>extract</code> multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money.</p>"},{"location":"writing/2023/11/26/python-caching/#1-functoolscache-for-simple-in-memory-caching","title":"1. <code>functools.cache</code> for Simple In-Memory Caching","text":"<p>When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions.</p> <pre><code>import functools\n\n@functools.cache\ndef extract(data):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ]\n    )\n</code></pre> <p>Changing the Model does not Invalidate the Cache</p> <p>Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.</p> <p>Now we can call <code>extract</code> multiple times with the same argument, and the result will be cached in memory for faster access.</p> <pre><code>import time\n\nstart = time.perf_counter() # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\") # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\n&gt;&gt;&gt; Time taken: 0.9267581660533324\n&gt;&gt;&gt; Time taken: 1.2080417945981026e-06 # (3)\n</code></pre> <ol> <li>Using <code>time.perf_counter()</code> to measure the time taken to run the function is better than using <code>time.time()</code> because it's more accurate and less susceptible to system clock changes.</li> <li>The second time we call <code>extract</code>, the result is returned from the cache, and the function is not called.</li> <li>The second call to <code>extract</code> is much faster because the result is returned from the cache!</li> </ol> <p>Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.</p> What is a decorator? <p>A decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.</p> <pre><code>def decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\") # (1)\n        result = func(*args, **kwargs)\n        print(\"Do something after\") # (2)\n        return result\n    return wrapper\n\n@decorator\ndef say_hello():\n    print(\"Hello!\")\n\nsay_hello()\n&gt;&gt;&gt; \"Do something before\"\n&gt;&gt;&gt; \"Hello!\"\n&gt;&gt;&gt; \"Do something after\"\n</code></pre> <ol> <li>The code is executed before the function is called</li> <li>The code is executed after the function is called</li> </ol>"},{"location":"writing/2023/11/26/python-caching/#2-diskcache-for-persistent-large-data-caching","title":"2. <code>diskcache</code> for Persistent, Large Data Caching","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory') # (1)\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel): # (2)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <ol> <li>We create a new <code>diskcache.Cache</code> instance to store the cached data. This will create a new directory called <code>my_cache_directory</code> in the current working directory.</li> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic in this example code</li> </ol> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!</p> <pre><code>import functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.patch(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation # (4)\n    if not issubclass(return_type, BaseModel): # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\" #  (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ]\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> <li>We use Pydantic's <code>model_validate_json</code> to deserialize the cached result into a Pydantic model.</li> <li>We use <code>inspect.signature</code> to get the function's return type annotation, which we use to validate the cached result.</li> </ol> <p>Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence.</p>"},{"location":"writing/2023/11/26/python-caching/#2-redis-caching-decorator-for-distributed-systems","title":"2. Redis Caching Decorator for Distributed Systems","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.</p> <pre><code>import redis\nimport functools\nimport inspect\nimport json\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\ncache = redis.Redis(\"localhost\")\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel): # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\" # (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ]\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> </ol> <p>Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.</p> <p>Looking carefully</p> <p>If you look carefully at the code above you'll notice that we're using the same <code>instructor_cache</code> decorator as before. The implementatino is the same, but we're using a different caching backend!</p>"},{"location":"writing/2023/11/26/python-caching/#conclusion","title":"Conclusion","text":"<p>Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead.</p> <p>If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>If you like the content check out our GitHub as give us a star and checkout the library.</p>"},{"location":"writing/2023/02/05/centaur-chess/","title":"Centuar Chess: AI as a Collaborative Partner","text":"<p>This is a experimental post, please leave feedback in the comments below</p> <p>This was a essay written by ChatGPT given a quick transcript of a 5 minute mono-logue. The goal was to see if I could use ChatGPT to write a blog post. I think it did a pretty good job, but I'll let you be the judge.</p> <p>It is my intention that by the end you'll understand that AI is not a threat to human intelligence, but rather a tool that can be used to augment human creativity and productivity.</p>"},{"location":"writing/2023/02/05/centaur-chess/#unraveling-the-history-of-technological-skepticism","title":"Unraveling the History of Technological Skepticism","text":"<p>Technological advancements have always been met with a mix of skepticism and fear:</p> <ul> <li>Telephone: Warned to disrupt face-to-face communication.</li> <li>Calculators: Feared for diminishing mental arithmetic skills.</li> <li>Typewriter: Criticized for degrading writing quality.</li> <li>Printing Press: Seen as a threat to manual script work.</li> <li>Written Word: Believed to weaken human memory.</li> </ul> <p>A Pattern of Progress</p> <p>Each technology, while initially met with resistance, has ultimately contributed positively to society. The telephone connected distant people, calculators simplified complex calculations, and the Internet democratized information.</p>"},{"location":"writing/2023/02/05/centaur-chess/#centaur-chess-embracing-human-ai-collaboration","title":"Centaur Chess: Embracing Human-AI Collaboration","text":"<p>The concept of Centaur Chess, emerging from Garry Kasparov's defeat by IBM's Deep Blue, illustrates a profound harmony between human strategy and AI's computational strength. It symbolizes the potential of human-AI collaboration, where AI tools augment human abilities without replacing them.</p> <p>The Essence of Centaur Chess</p> <p>\"You can do a lot with the computer, but you still have to play good chess.\" This quote reflects the complementary relationship between humans and AI, emphasizing that AI enhances but does not replace human skills.</p>"},{"location":"writing/2023/02/05/centaur-chess/#ai-tools-in-education-aiding-not-replacing","title":"AI Tools in Education: Aiding, Not Replacing","text":"<p>ChatGPT and similar AI tools in education should be seen as aids that raise the floor of literacy and writing skills without lowering the ceiling of excellence. They democratize access to quality writing, helping those with language barriers or communication difficulties to express themselves more effectively.</p> <p>Are AI Tools Diminishing Human Abilities?</p> <p>No, AI tools like ChatGPT are not threats but enablers that enhance human creativity and productivity. They provide assistance without detracting from the need for human insight and expertise.</p>"},{"location":"writing/2023/02/05/centaur-chess/#reflecting-on-the-ai-era-in-education","title":"Reflecting on the AI Era in Education","text":"<p>Recalling the early 2000s, educators warned against using Wikipedia, fearing its reliability. However, we learned to use it effectively, understanding the importance of verifying information and evaluating sources. This adaptability is essential in today's context with AI tools.</p>"},{"location":"writing/2023/02/05/centaur-chess/#the-broader-picture-ai-as-a-catalyst-for-improvement","title":"The Broader Picture: AI as a Catalyst for Improvement","text":"<p>Historically, each technological advancement, from calculators to the Internet, has been criticized but ultimately led to progress and improvement in human capabilities. AI in education is no different. It offers an opportunity to enhance learning and creativity, provided we approach it with the right mindset.</p>"},{"location":"writing/2023/02/05/centaur-chess/#conclusion-the-chess-game-with-ai","title":"Conclusion: The Chess Game with AI","text":"<p>Technology, including AI, has always raised concerns, but history shows it has augmented humanity, making us more productive and capable. Viewing AI as a collaborative partner can lead to a more enriched human experience. In the game of technology and AI, \"we still have to play good chess,\" leveraging these tools to elevate our skills and creativity, rather than viewing them as replacements for human intelligence.</p>"},{"location":"writing/2023/02/05/freediving-ice/","title":"Freediving under ice","text":"<p>Growing up, I wasn't very physically active. However, as I got older and had more time, I made a conscious effort to get in shape and improve my relationship with my body. During the Covid pandemic, I developed RSI and my thumbs from coding too much, which prevented me from participating in any sports.</p> <p></p> <p>Determined to stay active, I turned to swimming and found myself particularly drawn to the breath-hold aspect of the sport. With some training and certifications under my belt, I was eventually invited to take part in an ice dive in Canada. It was a surreal and thrilling experience, with the freezing water and the sound of my heartbeat in my ears as I descended deeper and deeper.</p> <p>Free diving has become an integral part of my life, providing me with a sense of freedom and exploration that I never thought possible. It has allowed me to push myself both physically and mentally, and has given me a deeper appreciation for the ocean and its inhabitants.</p>"},{"location":"writing/2023/06/01/kojima-sticks/","title":"Kojima's Philosophy in LLMs: From Sticks to Ropes","text":"<p>Hideo Kojima's unique perspective on game design, emphasizing empowerment over guidance, offers a striking parallel to the evolving world of Large Language Models (LLMs). Kojima advocates for giving players a rope, not a stick, signifying support that encourages exploration and personal growth. This concept, when applied to LLMs, raises a critical question: Are we merely using these models as tools for straightforward tasks, or are we empowering users to think critically and creatively?</p> <p>Kojima eloquently described the evolution of tools: \"The stick was the first tool, created to maintain distance from threats. The rope followed, symbolizing connection and protection of valued entities.\" In gaming, this translates to a shift from aggressive, direct actions ('sticks') to collaborative and connective tools ('ropes'). This philosophy in LLMs should aim to not just deliver information but to foster a deeper level of engagement and creative thinking.</p>"},{"location":"writing/2023/06/01/kojima-sticks/#empowering-users-a-shift-in-llm-applications","title":"Empowering Users: A Shift in LLM Applications","text":"<p>Are LLMs Just Advanced Answering Machines?</p> <p>The prevalent use of LLMs for tasks like question answering, content generation, and summarization risks overshadowing their potential in promoting critical thinking and creativity. LLMs should be more than just advanced answering machines; they should act as catalysts for intellectual growth and idea generation.</p> <p>Guiding users in their creative and intellectual endeavors aligns with the 'rope' philosophy. It's about shifting from delivering ready-made content to nurturing unique thought processes and writing styles, with the LLM playing a supportive, guiding role.</p> <p>Drawing from my experience as a senior engineer, I recall moments when junior engineers sought straightforward answers. Offering direct solutions was easy, but guiding them to find answers on their own was crucial for their development. Similarly, LLMs should aim to teach and guide rather than just provide.</p>"},{"location":"writing/2023/06/01/kojima-sticks/#rethinking-user-engagement-with-llms","title":"Rethinking User Engagement with LLMs","text":"<p>Here are two illustrative scenarios demonstrating how LLMs can empower users:</p>"},{"location":"writing/2023/06/01/kojima-sticks/#study-notes-app","title":"\ud83d\udcda Study Notes App","text":"Agency Level Description Low Agency A basic app summarizing information into pre-defined guides. High Agency An advanced app generating open-ended, personalized questions; guiding users to sources and encouraging exploration of related topics."},{"location":"writing/2023/06/01/kojima-sticks/#journaling-app","title":"\ud83d\udcd4 Journaling App","text":"Agency Level Description Low Agency A basic app for transcribing voice memos. High Agency An interactive app posing thought-provoking questions, offering feedback, and encouraging users to delve deeper into their thoughts. <p>Harnessing LLMs for Creative Empowerment</p> <p>The true potential of LLMs lies in transforming passive content consumption into active learning and idea generation. By embracing Kojima's philosophy of providing ropes instead of sticks, we can redefine the role of LLMs in our intellectual and creative journeys.</p> <p>In conclusion, LLMs possess the potential to revolutionize learning and communication. However, the current trend leans towards passive use. By adopting a more empowering approach, encouraging active engagement and creativity, we can unlock their true potential. Let's shift our focus from mere content generation to fostering a deeper level of intellectual and creative engagement, embracing Kojima's vision in the realm of LLMs.</p>"},{"location":"writing/2023/04/04/good-llm-observability/","title":"Good LLM Observability is just plain observability","text":"<p>In this post, I aim to demystify the concept of LLM observability. I'll illustrate how everyday tools employed in system monitoring and debugging can be effectively harnessed to enhance AI agents. Using Open Telemetry, we'll delve into creating comprehensive telemetry for intricate agent actions, spanning from question answering to autonomous decision-making.</p> <p>What is Open Telemetry?</p> <p>Essentially, Open Telemetry comprises a suite of APIs, tools, and SDKs that facilitate the creation, collection, and exportation of telemetry data (such as metrics, logs, and traces). This data is crucial for analyzing and understanding the performance and behavior of software applications.</p>"},{"location":"writing/2023/04/04/good-llm-observability/#demystifying-telemetry-in-ai","title":"Demystifying Telemetry in AI","text":"<p>The lack of sufficient observability in many AI agents today hinders their evaluation and optimization in real-world scenarios. By integrating Open Telemetry, we can not only enhance the transparency of these agents through tools like Prometheus, Grafana, and Datadog, but also reincorporate this insight to refine the agents themselves.</p> <p>However, it's crucial to recognize that what's often marketed as specialized LLM telemetry services are merely superficial dashboards encapsulating basic API interactions. These don't provide the depth required for generating extensive telemetry across the whole stack or the means to meaningfully reintegrate this data into the AI agents.</p>"},{"location":"writing/2023/04/04/good-llm-observability/#applying-telemetry-to-ai-agents","title":"Applying Telemetry to AI Agents","text":"<p>Consider a conversational agent that formulates SQL queries in response to natural language inquiries, interacting with various data sources through a Router Agent. If issues arise, be it database errors or latency spikes, pinpointing the culprit - whether the LLM, the SQL query, or the database itself - becomes challenging. Current LLM operations rarely offer comprehensive instrumentation of external components, leaving these questions unanswered.</p> <p>Adopting standards like Open Telemetry can bridge this gap, offering a holistic view of the agent's actions and their interconnections. This insight is pivotal for enhancing system performance, robustness, and incident detection and resolution.</p>"},{"location":"writing/2023/04/04/good-llm-observability/#the-potential-of-telemetry-data","title":"The Potential of Telemetry Data","text":"<p>Envision utilizing telemetry data for model-guided self-evaluation. This approach could revolutionize scalable model evaluation. By analyzing the complete task call graph, we can identify and address inefficiencies - for instance, isolating events leading to high-latency database calls or errors.</p> <p>This data, once fed back into the LLM, could prompt targeted fine-tuning. The LLM might analyze a series of transactions, identifying and ranking documents based on relevance, or suggest corrections in a cycle of calls, thus refining the data for model improvement.</p>"},{"location":"writing/2023/04/04/good-llm-observability/#redefining-telemetry-the-key-to-self-improvement","title":"Redefining Telemetry: The Key to Self-Improvement?","text":"<p>Telemetry in the realm of AGI might well be akin to a detailed diary, instrumental for reflection and advancement. With robust telemetry, we gain unprecedented insight into the actions of AI agents, enabling the creation of systems that not only evaluate but also self-optimize complex actions. This synergy of human and computer intelligence, driven by comprehensive telemetry, holds the key to unlocking the full potential of AI systems.</p> <p>In essence, observing LLM systems doesn't necessitate new tools; it requires viewing agent systems through the lens of distributed systems. The distinction lies in the potential exportation of this data for the refinement and distillation of other models.</p> <p>A prime example of this distillation process can be found in the Instructor blog, where a GPT-3.5 model is fine-tuned using GPT-4 outputs, demonstrating the power of leveraging telemetry data for model enhancement.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/","title":"How to build a terrible RAG system","text":"<p>If you've seen any of my work, you know that the main message I have for anyone building a RAG system is to think of it primarily as a recommendation system. Today, I want to introduce the concept of inverted thinking to address how we should approach the challenge of creating an exceptional system.</p> <p>What is inverted thinking?</p> <p>Inversion is the practice of thinking through problems in reverse. It's the practice of \u201cinverting\u201d a problem - turning it upside down - to see it from a different perspective. In its most powerful form, inversion is asking how an endeavor could fail, and then being careful to avoid those pitfalls. [1]</p> <p>Inventory</p> <p>You'll often see me use the term inventory. I use it to refer to the set of documents that we're searching over. It's a term that I picked up from the e-commerce world. It's a great term because it's a lot more general than the term corpus. It's also a lot more specific than the term collection. It's a term that can be used to refer to the set of documents that we're searching over, the set of products that we're selling, or the set of items that we're recommending.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#dont-worry-about-latency","title":"Don't worry about latency","text":"<p>There must be a reason that chat GPT tries to stream text out. Instead, we should only show the results once the entire response is completed. Many e-commerce websites have found that 100 ms improvement in latency can increase revenue by 1%. Check out  How One Second Could Cost Amazon $1.6 Billion In Sales.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#dont-show-intermediate-results","title":"Don't show intermediate results","text":"<p>Users love love staring at a blank screen. It's a great way to build anticipation. If we communicated intermittent steps like the ones listed below, we'd just be giving away the secret sauce and users prefer to be left in the dark about what's going on.</p> <ol> <li>Understanding your question</li> <li>Searching with \"...\"</li> <li>Finding the answer</li> <li>Generating response</li> </ol>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#dont-show-them-the-source-document","title":"Don't Show Them the Source Document","text":"<p>Never show the source documents, and never highlight the origin of the text used to generate the response. Users should never have to fact-check our sources or verify the accuracy of the response. We should assume that they trust us and that there is no risk of false statements.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-not-worry-about-churn","title":"We Should Not Worry About Churn","text":"<p>We are not building a platform; we are just developing a machine learning system to gather metrics. Instead of focusing on churn, we should concentrate on the local metrics of our machine learning system like AUC and focus on benchmarks on HuggingFace.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-use-a-generic-search-index","title":"We Should Use a Generic Search Index","text":"<p>Rather than asking users or trying to understand the types of queries they make, we should stick with a generic search and not allow users to generate more specific queries. There is no reason for Amazon to enable filtering by stars, price, or brand. It would be a waste of time! Google should not separate queries into web, images, maps, shopping, news, videos, books, and flights. There should be a single search bar, and we should assume that users will find what they're looking for.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-not-develop-custom-ui","title":"We Should Not Develop Custom UI","text":"<p>It doesn't make sense to build a specific weather widget when the user asks for weather information. Instead, we should display the most relevant information. Semantic search is flawless and can effectively handle location or time-based queries. It can also re-rank the results to ensure relevance.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-not-fine-tune-our-embeddings","title":"We Should Not Fine-Tune Our Embeddings","text":"<p>A company like Netflix should have a generic movie embedding that can be used to recommend movies to people. There's no need to rely on individual preferences (likes or dislikes) to improve the user or movie embeddings. Generic embeddings that perform well on benchmarks are sufficient for building a product.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-train-an-llm","title":"We Should Train an LLM","text":"<p>Running inference on a large language model locally, which scales well, is cost-effective and efficient. There's no reason to depend on OpenAI for this task. Instead, we should consider hiring someone and paying them $250k a year to figure out scaling and running inference on a large language model. OpenAI does not offer any additional convenience or ease of use. By doing this, we can save money on labor costs.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-not-manually-curate-our-inventory","title":"We Should Not Manually Curate Our Inventory","text":"<p>There's no need for manual curation of our inventory. Instead, we can use a generic search index and assume that the documents we have are relevant to the user's query. Netflix should not have to manually curate the movies they offer or add additional metadata like actors and actresses to determine which thumbnails to show for improving click rates. The content ingested on day one is sufficient to create a great recommendation system.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-not-analyze-inbound-queries","title":"We Should Not Analyze Inbound Queries","text":"<p>Analyzing the best and worst performing queries over time or understanding how different user cohorts ask questions will not provide any valuable insights. Looking at the data itself will not help us generate new ideas to improve specific segments of our recommendation system. Instead, we should focus on improving the recommendation system as a whole and avoid specialization.</p> <p>Imagine if Netflix observed that people were searching for \"movies with Will Smith\" and decided to add a feature that allows users to search for movies with Will Smith. That would be a waste of time. There's no need to analyze the data and make system improvements based on such observations.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#machine-learning-engineers-should-not-be-involved-in-ingestion","title":"Machine Learning Engineers Should Not Be Involved in Ingestion","text":"<p>Machine Learning Engineers (MLEs) do not gain valuable insights by examining the data source or consulting domain experts. Their role should be limited to working with the given features. Theres no way that MLEs who love music would do a better job at Spotify, or a MLE who loves movies would do a better job at Netflix. Their only job is to take in data and make predictions.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-use-a-knowledge-graph","title":"We Should Use a Knowledge Graph","text":"<p>Our problem is so unique that it cannot be handled by a search index and a relational database. It is unnecessary to perform 1-2 left joins to answer a single question. Instead, considering the trending popularity of knowledge graphs on Twitter, it might be worth exploring the use of a knowledge graph for our specific case.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-treat-all-inbound-inventory-the-same","title":"We should treat all inbound inventory the same","text":"<p>There's no need to understand the different types of documents that we're ingesting. How differnet could marketting content, construction documnets, and energy bills be? Just becase some have images, some have tables, and some have text doesn't mean we should treat them differently. We its all text, and so an LLM should just be able to handle it.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-not-have-to-build-special-injestion-pipelines","title":"We should not have to build special injestion pipelines","text":"<p>GPT-4 has solve all of data processing so if i handle a photo album, a pdf, and a word doc, it should be able to handle any type of document. There's no need to build special injestion pipelines for different types of documents. We should just assume that the LLM will be able to handle it. I shouldn't dont even have to think about what kinds of questions I need to answer. I should just be able to ask it anything and it should be able to answer it.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-never-have-to-ask-the-data-provider-for-clean-data","title":"We should never have to ask the data provider for clean data","text":"<p>If Universal studios gave Netflix a bunch of MOV files with no metadata, Netflix should not have to ask Universal studios to provide additional movie metadata. Universal might not know the runtime, or the cast list and its netflix's job to figure that out. Universal should not have to provide any additional information about the movies they're providing.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-never-have-to-cluster-our-inventory","title":"We should never have to cluster our inventory","text":"<p>Theres only one kind of inventory and one kind of question. We should just assume that the LLM will be able to handle it. I shouldn't dont even have to think about what kinds of questions I need to answer. Topic clustering would only show us how uniform our inventory is and how little variation there is in the types of questions that users ask.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#we-should-focus-on-local-evals-and-not-ab-tests","title":"We should focus on local evals and not A/B tests","text":"<p>Once we run our GPT-4 self critique evaluations we'll know how well our system is doing and it'll make us more money, We should spend most of our time writing evaluation prompts and measuring precision / recall and just launching the best one. A/B tests are a waste of time and we should just assume that the best performing prompt will be the best performing business outcome.</p>"},{"location":"writing/2024/01/07/inverted-thinking-rag/#conclusion","title":"Conclusion","text":"<p>Hope you enjoyed some of these tips. Now, your job is to take each one and invert it. Let me know how it goes! If you like this, please consider resharing it on twitter or just tweet at me and let me know if theres any more to add.</p>"},{"location":"writing/2023/09/17/rag-is-more-than-embeddings/","title":"RAG is more than just embedding search","text":"<p>With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.</p> <p>What is RAG?</p> <p>Retrival augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized.</p> <p> </p> Simple RAG that embedded the user query and makes a search. <p>So let's kick things off by examining what I like to call the 'Dumb' RAG Model\u2014a basic setup that's more common than you'd think.</p>"},{"location":"writing/2023/09/17/rag-is-more-than-embeddings/#the-dumb-rag-model","title":"The 'Dumb' RAG Model","text":"<p>When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like <code>search(query: str) -&gt; List[str]</code>. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings.</p>"},{"location":"writing/2023/09/17/rag-is-more-than-embeddings/#why-is-this-a-problem","title":"Why is this a problem?","text":"<ul> <li> <p>Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation!</p> </li> <li> <p>Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more.</p> </li> <li> <p>Limitation of text search: Restricts complex queries to a single string (<code>{query: str}</code>), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking <code>what problems did we fix last week</code> cannot be answered by a simple text search since documents that contain <code>problem, last week</code> are going to be present at every week.</p> </li> <li> <p>Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results.</p> </li> </ul> <p>Now let's dive into how we can make it smarter with query understanding. This is where things get interesting.</p>"},{"location":"writing/2023/09/17/rag-is-more-than-embeddings/#improving-the-rag-model-with-query-understanding","title":"Improving the RAG Model with Query Understanding","text":"<p>Shoutouts</p> <p>Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out!</p> <p>Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall.</p> <p> </p> Query Understanding system routes to multiple search backends. <p>Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems.</p>"},{"location":"writing/2023/09/17/rag-is-more-than-embeddings/#whats-instructor","title":"Whats instructor?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul>"},{"location":"writing/2023/09/17/rag-is-more-than-embeddings/#case-study-1-metaphor-systems","title":"Case Study 1: Metaphor Systems","text":"<p>Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query.</p> <p></p> Metaphor Systems UI <p>If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n\nclass MetaphorQuery(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n    domains_allow_list: List[str]\n\n    async def execute():\n        return await metaphor.search(...)\n</code></pre> <p>Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.patch(OpenAI())\n\nquery = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=MetaphorQuery,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What are some recent developments in AI?\"\n        }\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n  \"rewritten_query\": \"novel developments advancements ai artificial intelligence machine learning\",\n  \"published_daterange\": {\n    \"start\": \"2023-09-17\",\n    \"end\": \"2021-06-17\"\n  },\n  \"domains_allow_list\": [\"arxiv.org\"]\n}\n</code></pre> <p>This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features.</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n    chain_of_thought: str = Field(\n        None,\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n</code></pre> <p>Now, let's see how this approach can help model an agent like personal assistant.</p>"},{"location":"writing/2023/09/17/rag-is-more-than-embeddings/#case-study-2-personal-assistant","title":"Case Study 2: Personal Assistant","text":"<p>Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts.</p> <pre><code>class ClientSource(enum.Enum):\n    GMAIL = \"gmail\"\n    CALENDAR = \"calendar\"\n\nclass SearchClient(BaseModel):\n    query: str\n    keywords: List[str]\n    email: str\n    source: ClientSource\n    start_date: datetime.date\n    end_date: datetime.date\n\n    async def execute(self) -&gt; str:\n        if self.source == ClientSource.GMAIL:\n            ...\n        elif self.source == ClientSource.CALENDAR:\n            ...\n\nclass Retrival(BaseModel):\n    queries: List[SearchClient]\n\n    async def execute(self) -&gt; str:\n        return await asyncio.gather(*[query.execute() for query in self.queries])\n</code></pre> <p>Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.patch(OpenAI())\n\nretrival = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Retrival,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"},\n        {\"role\": \"user\", \"content\": \"What do I have today?\"}\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n    \"queries\": [\n        {\n            \"query\": None,\n            \"keywords\": None,\n            \"email\": \"jason@example.com\",\n            \"source\": \"gmail\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n        },\n        {\n            \"query\": None,\n            \"keywords\": [\"meeting\", \"call\", \"zoom\"]]],\n            \"email\": \"jason@example.com\",\n            \"source\": \"calendar\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n\n        }\n    ]\n}\n</code></pre> <p>Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app.</p> <p>Can I used framework X?</p> <p>I get this question a lot, but it's just code. Within these dispatchs you can do whatever you want. You can use <code>input()</code> to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit.</p> <p>Both of these examples showcase how both search providors and consumers can use <code>instructor</code> to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend.</p>"},{"location":"writing/2023/09/17/rag-is-more-than-embeddings/#conclusion","title":"Conclusion","text":"<p>This isnt about fancy embedding tricks, it's just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place.</p>"},{"location":"writing/2023/09/17/rag-is-more-than-embeddings/#whats-next","title":"What's Next?","text":"<p>Here I want to show that `instructor`` isn\u2019t just about data extraction. It\u2019s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning \u2014 the untapped goldmine is skilled use of tools and APIs.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>"},{"location":"writing/2022/08/01/stitchfix-framework/","title":"Recommendations with Flight at Stitch Fix","text":"<p>As a data scientist at Stitch Fix, I faced the challenge of adapting recommendation code for real-time systems. With the absence of standardization and proper performance testing, tracing, and logging, building reliable systems was a struggle.</p> <p>To tackle these problems, I created Flight \u2013 a framework that acts as a semantic bridge and integrates multiple systems within Stitch Fix. It provides modular operator classes for data scientists to develop, and offers three levels of user experience.</p> <ul> <li>The pipeline layer allows business-knowledge users to define pipelines in plain English.</li> <li>The operator layer enables data scientists to add and share many filters and transformations with ease.</li> <li>The meta layer provides platform engineers the ability to introduce new features without affecting the development experience of data scientists.</li> </ul> <p>Flight improves the \"bus factor\" and reduces cognitive load for new developers, standardizes logging and debugging tools, and includes advanced distributed tracing for performance measurement and metrics monitoring.</p>"},{"location":"writing/2022/08/01/stitchfix-framework/#pipeline-layer","title":"Pipeline Layer","text":"<p>The <code>Pipeline</code> class is the foundation of the Flight framework, enabling users with business domain knowledge to craft pipelines composed of a variety of modular operators. The resulting code is readable and almost resembles plain English. The code sample below showcases how the <code>Pipeline</code> class can be used to set inclusion and exclusion criteria and scoring functions for a given item type.</p> <pre><code>from flight.pipelines import Pipeline\n\nimport flight.sourcing as so\nimport flight.scoring as sf\nimport flight.operators as fo\n\n@app.post(\"/recs/complimentary_items\")\nasync def complimentary_items(client_id: int, product_id:int):\n    pipeline = Pipeline(\"complimentary_items\").initialize(\n       includes=[so.AvailableInventory(), so.MatchClientSize()]\n       excludes=[so.PreviouslyPurchased()]\n       scores=[sc.ProbabilityOfSale(\"psale_score\"),\n      item_type=\"sku_id\",\n    )\n\n    pipeline = (pipeline\n   | fo.Hydrate([\"department\", \"product_id\"])\n   | fo.MatchDepartment(product_id)\n   | fo.DiverseSample(n=10, maximize=\"psale_score\")\n   | fo.Sort(\"score\" desc=True)\n    )\n\n  # pipelines are lazy so stuff only happens on execute()\n    resp = await pipeline.execute(\n        client_id, return_cols=[\"sku_id\", \"product_id\", \"score\"], **kwargs\n    )\n    return resp\n</code></pre> <p>In the shopping example, we start by performing the set operation <code>Union(includes) - Union(excludes)</code> and then calculate scores for the results. It's worth taking a look at the code to get a better understanding of how it works on first glance. The pipeline class manages the whole process, allowing us to have control over how best to compute.</p>"},{"location":"writing/2022/08/01/stitchfix-framework/#operator-layer","title":"Operator Layer","text":"<p>Operators in the framework are implemented as classes, with static variables defined using the <code>dataclass</code> style, and dynamic variables passed in during runtime. For example, <code>SourceOperators</code> such as the <code>Inventory</code> operator rely on external APIs to retrieve data, while <code>IndexOperators</code> like <code>MatchDepartment</code> merely return indices, providing an efficient way to manage pipelines without mutating dataframes.</p> <pre><code>class AvailableInventory(fo.SourceOperator):\n   async def __call__(self, **kwargs) -&gt; fo.Source:\n       data = await get_inventory(**kwargs)\n       return fo.Source(data)\n\nclass MatchDepartment(fo.FilterOperator)\n    product_id: int\n    department: str\n\n    def __call__(self, df, **kwargs) -&gt; pd.Index:\n        assert \"department\" in df.columns\n        department = get_product(self.product_id, \"department\")\n    self.department = department\n    return df[df.department == department].index\n</code></pre>"},{"location":"writing/2022/08/01/stitchfix-framework/#meta-layer","title":"Meta Layer","text":"<p>In the pipeline layer, you only have to worry about the shape of the pipeline, not pandas code required. In the operator you only need to make sure your pandas or etc code fits the shape of the signature. Return a <code>fo.Source</code> or a <code>pd.Index</code> and all data merging, filter, augmentation happens behinds the scenes.</p> <p>So what actually happens?</p>"},{"location":"writing/2022/08/01/stitchfix-framework/#error-handling","title":"Error handling:","text":"<p>Pipeline handles errors on <code>execute</code>, providing info on what went wrong. Since errors only occur in <code>__call__</code> method of operator, making it easy to write tests to catch errors and identify the operator causing the issue. This especially useful when we don't know why no recommendations were generated.</p> <pre><code># not an error, just due to the pipeline\nresp = {\n   product_id=[],\n   error=False,\n   reason=\"MatchDepartment(product_id=3) pipeline returned 0 items after filtering 53 items\"\n}\n# actual error, since not having inventory is likely a systems issue and not an\nresp = {\n   product_id=[],\n   error=True,\n   reason=\"Inventory(warehouse_id=1) timed out after retres\"\n}\n</code></pre>"},{"location":"writing/2022/08/01/stitchfix-framework/#logging","title":"Logging","text":"<p>Operators are logged at various levels of detail. When <code>initialize</code> is called, we log each class that was called, the number of results produced, and information on how data was intersected and combined. Each log is structured with the <code>dataclass</code>level information of each operato</p> <pre><code>&gt; Inventory(warehouse=\"any\") returned 5002 products in 430ms\n&gt; MatchSize(\"S\") returned 1231 products in 12ms\n&gt; After initalization, 500 products remain\n&gt; MatchDepartment(product_id=3) filtered 500 items to 51 items in 31ms\n&gt; Diversity(n=10) filtered 51 items to 10 items in 50ms\n&gt; Returning 10 items with mean(score)=0.8\n</code></pre> <p>By injesting this data into something like Datadog we can add monitors on our operators, the results, the distribution of results.</p>"},{"location":"writing/2022/08/01/stitchfix-framework/#distributed-tracing","title":"Distributed Tracing","text":"<p>With integration of OpenTelemetry's tracing logic, Flight allows for comprehensive tracing of each operator, providing visibility into performance issues from end to end. This is particularly useful for source operators.</p>"},{"location":"writing/2022/08/01/stitchfix-framework/#dynamic-execution","title":"Dynamic Execution","text":"<p>The entire pipeline object is around passing around classes with <code>dataclass</code>style initialization. This simple fact that all arguments tend to be primitives allows us to create pipelines dynamically, either through config or requests, you could imagine a situation where it might be useful to define pipelines by config like JSON or YAML and have an engine serve many many pipelines dynamically</p> <pre><code># config.yaml\npipeline:\n  name: \"MyPipeline\"\n  item_type: \"sku_id\"\n  initialization:\n    includes:\n    - name: AvailableInventory\n    scorer:\n    - name: ClickRate\n\n  operations:\n  - name: Sort\n    parameters:\n      score: \"click_rate\"\n      desc: True\n\n# run.py\n@app.post(\"/execute_config\")\nasync def execute(config, kwargs):\n   pipeline = Pipeline.from_config(config)\n   return await pipeline.execute(**kwargs)\n\n@app.post(\"/execute_name\")\nasync def execute_from_config(name, kwargs)\n   config = get_config(name)\n   return await execute(config, kwargs)\n</code></pre>"},{"location":"writing/2022/08/01/stitchfix-framework/#debugging","title":"Debugging","text":"<p>Debugging data quality issues or identifying the reasons behind clients not being able to see inventory can be a challenge. Flight's verbose mode allows for detailed debugging by listing products and viewing the index at each step of the pipeline's iteration. This standardized debug output enables the creation of UI tools to explore results, compare operators, and analyze pipelines.</p> <pre><code># with verbose = debug = true\nresp = {\n   \"product_id\": [1,2,3]\n   \"debug\": {\n       \"includes\": [\n                    {\"name\": \"Inventory\", \"kwargs\":{}, \"product_ids\"=[1,2,3, ...]}]\n       \"excludes\": []\n       \"pipeline_operators\":\n          {\"name\": \"Match\", \"kwargs\":{...},\n                            \"input_ids\":[1,2,3, ...], \"n_input\": 100\n                            \"output_ids\":[1,2,3, ...}, \"n_output\": 400\n                    } ...\n   }\n}\n</code></pre> <p>The capabilities provided by the glue of the meta layer allowed us to systematically inspect pipelines and operators, identify bottlenecks in our micro services, and directly communicate with other teams to improve performance and latency.</p>"},{"location":"writing/2022/08/01/stitchfix-framework/#conclusion","title":"Conclusion","text":"<p>Flight has been a tremendous asset for managing data pipelines at Stitch Fix. The pipeline architecture, employing the source and index operator pattern, has made it simpler to write maintainable code and rapidly detect performance issues. The monitoring capabilities of OpenTelemetry's integrated distributed tracing have also been in valuable for ensuring efficient pipeline execution and debugging.</p> <p>As the number of pipelines and operators increases, it may be necessary to look into more scalable solutions for managing the execution. Nevertheless, the current architecture has been more than adequate so far, and the emphasis has been on creating practical solutions that meet the business requirements.</p>"},{"location":"writing/2024/01/01/whoami/","title":"Who am I?","text":"<p>In the next year, this blog will be painted with a mix of technical machine learning content and personal notes. I've spent more of my 20s thinking about my life than machine learning. I'm not good at either, but I enjoy both.</p>"},{"location":"writing/2024/01/01/whoami/#life-story","title":"Life story","text":"<p>I was born in a village in China. My parents were the children of rural farmers who grew up during the Cultural Revolution. They were the first generation of their family to read and write, and also the first generation to leave the village.</p> <p>Growing up, I had always been interested in art and science. In high school, I went to a public art school and studied digital animation and design for 4 years. Then, in college, I went to Waterloo to study mathematical physics. By the second year, I met a guy named TC who was clearly much smarter than me, so I decided to do something else. At that time, I learned to code through dating this girl Christine. I took Andrew Ng's machine learning course and got hooked.</p> <p>Physics timelines were too long for me, so I switched to machine learning and focused my interests on social networks and understanding how humans interact with each other. In the process, I generated massive volumes of data that could be used to train models and understand human behavior. This led me to do the research I had done at NYU and then at Facebook.</p> <p>I thought my dream would have been to study computational social science at Facebook, but due to the politics around online safety, the role of the data scientist at the organization, and the bloat of middle management, I decided that I didn't want to be part of the system, so I left. And joined my mentor Chris Moody and my friend Thomas Miller at Stitch Fix.</p> <p>There, I spent the first 2 years working on vision-based models, fine-tuning multimodal embedding models for clothing and fashion understanding, a range of classification, retrieval, bounding box, and recommendation system problems. As I became more senior and the team grew, I started to focus more on the infrastructure and platform side of things. I built a framework called Flight, which is a framework for building and executing pipelines. It's a semantic bridge that integrates multiple systems within Stitch Fix. It provides modular operator classes for data scientists to develop and offers three levels of user experience. This really taught me what I use today to inform a lot of my thinking around building tools.</p>"},{"location":"writing/2024/01/01/whoami/#ouch","title":"Ouch","text":"<p>Near the end of 2020, I was diagnosed with RSI and had to take a break from work. I spent the next 6 months recovering and learning to live with the pain. I had likely worked too much and too hard, and my body was telling me to slow down. I'm still recovering, but I'm doing much better now.</p>"},{"location":"writing/2024/01/01/whoami/#crisis","title":"Crisis","text":"<p>What it did do, that I'm grateful for, is it got me to slow down. In particular, being the first son of the first son and also of an immigrant family, I had always felt the need to prove myself. I had always felt the need to be the best, to be the smartest, to be the most successful. And by losing my ability to work, and by proxy (although I did not lose my job), make money and be productive, I was forced to confront my own self-worth.</p> <p>It was a very interesting 2 years. Between 2020 and the latter half of 2023, I would often have constant pain in both my hands that prevented me from working, working out, eating properly, and doing normal everyday things. I had to learn to live with the pain, and I had to learn to be okay with not being productive.</p> <p>I really struggled for a long time with what it means to feel deserving of love. It didn't occur to me that much of my confidence and self-worth was tied to my ability to be productive.</p> <p>Even now, I live with some fear that the pain will come back. I'm hesitant to work too much, and I'm hesitant to work too hard. Instead of raising money or being a founder, I've found a niche advising startups while giving myself time to still examine life and think about what it means to be a person.</p> <p>Now I'm in a much better place. I'm still learning to live with the pain, but I'm also learning to be okay with not being productive. I enjoy martial arts, pottery, free diving, and just messing around on the internet.</p> <p>I've also found a newfound role as a writer. Someone said that if you died and never wrote anything down, it was a wasted life. It's hyperbolic, but I understood what that meant.</p> <p>So I write with no predefined purpose, but I just want to put some stuff on a page and see if it becomes something else.</p>"},{"location":"writing/archive/2024/","title":"2024","text":""},{"location":"writing/archive/2023/","title":"2023","text":""},{"location":"writing/archive/2022/","title":"2022","text":""},{"location":"writing/category/personal/","title":"Personal","text":""},{"location":"writing/category/language-models/","title":"Language Models","text":""},{"location":"writing/category/retrieval-augmented-generation/","title":"Retrieval Augmented Generation","text":""},{"location":"writing/category/query-understanding/","title":"Query Understanding","text":""},{"location":"writing/category/search-systems/","title":"Search Systems","text":""},{"location":"writing/category/python/","title":"Python","text":""},{"location":"writing/category/observability/","title":"Observability","text":""},{"location":"writing/category/llm/","title":"LLM","text":""},{"location":"writing/category/stitch-fix/","title":"Stitch Fix","text":""},{"location":"writing/category/data-science/","title":"Data Science","text":""},{"location":"writing/category/frameworks/","title":"Frameworks","text":""},{"location":"writing/page/2/","title":"Writing and mumblings","text":""}]}